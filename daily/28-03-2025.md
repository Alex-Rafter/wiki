# Ideas on Sprint 1 for New World / BP Static

Over-arching Aim for Sprint 1
Production ready 'back of the front' for new codebase: Able to fetch, edit, persist, and make use of data from interim endpoints. We should be able to trigger these steps manually, and via schedules and POST requests.

3 Objectives: 
1. JSON returning interim endpoints - finished.
2. Scripted data fetching, editing, and storing as local content store - finished. 
3. Data loading into Astro templates with core (Astro-level) API design in place - finished
## What that means in practice : 
1. BP JSON - finished. 
	- All 'endpoints' needed completed in full and returning data in a coherent structure.
	- Any gaps identified eg for Tech work etc. 
2. Data Fetching to Local Static Files - finished. 
	- All endpoint data can be fetched from endpoints - on script run.
	- All data can be correctly translated into format needed for content store - on script run
	- All edited data (JSON) stored locally - as flat file content store / db  - on script run
	- GitHub Actions workflows set up to trigger these script runs on POST.
	- Additional GitHub Actions workflows set up to send the POST req eg on schedule or workflow dispatch (to mimic any future Tech webhook), etc
 3.  Data loading into Astro templates - finished
	- All data from local content store can be loaded in to the components 
	- We have a consistent consistent and simple API for data loading from our content store into components
	- Demos to show of pulling this data in and showing on the front end*

Notes
- I've set stretch goals for Sprint 1. Not sure if i am being a bit ambitious. But i'll let you know how we go!
- I've scoped 'done' as being production ready code in the areas above, with some initial docs. But i think its going to be good to look at some additional time down the line specifically to extend those and make them as accessible as poss.
- *These will be bare-bones form a UI POV as the focus for Sprint 1 is data fetching, editing, persistence, and api design over UI or template work

## LLM learning and building Opps

BP JSON: 
- Feed examples created so far as context / constraints
- Build out data structures for each endpoint with LLMs - using example as context 
- Build out html partial with script tag and JSON from the literals / cms docs provided 
- Should be able to build all of these quite quickly? 

Data Fetching to Local Static Files: 
- Feed it my config object and get feedback / alternatives / tighten this up a bit

## Extensions / Ideas for Each: 

BP JSON: 
- List out what all the necessary endpoints should be first
- Build out high level endpoints first. 
- Diagram these endpoints into a tree (TD)
- This should link with data fetch - so, data from high level dictates next fetches - eg
	- fetch website endpoint data : name, location ids, etc
	-  Not 100% sure we even need this? Probs for CMS more than COG data.

Data Fetching to Local Static Files: 
- Trigger fetch from workflow - on post. 
- Fetch sub-sets of data based on workflow trigger / args passed when triggering script
- Update to use async / promises etc so that all the data is fetched and edited more quickly


Names
BP JSON
Data store create
Data store use

Steps: 
1. API / BP JSON First 
Diagram / plan out tree. Do for COG and for CMS
Start at top and work down?
Update existing to use more literals / all literals 

Deliverables
- Tree diagram x2 
- All template files returning structured data - html with script tags with type tex/json 

2. Data Fetching to Local Static Files

Deliverables 
- Script that can fetch all the data from the endpoints and store as plan text db
- Optimised with async to be as fast as possible 
- Data tested / handling errors and exceptions (get advice / use LLMs to guide)
- Build partial fetches based on args passed.
- Hook it up to GH Workflows to trigger on post
- Add additional workflow to dispatch post - so this can be switched out for webhooks 
